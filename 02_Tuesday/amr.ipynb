{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32234c49-297f-44ec-87df-92f8cbb285a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spectre\n",
    "import spectre.IO.H5 as spectre_h5\n",
    "from spectre.Spectral import Mesh, Basis, Quadrature, logical_coordinates\n",
    "from spectre.DataStructures import DataVector, Index\n",
    "from spectre.DataStructures.Tensor import InverseJacobian, Jacobian, Scalar, tnsr\n",
    "from spectre.NumericalAlgorithms.LinearOperators import power_monitors\n",
    "from spectre.Domain.Creators import Interval\n",
    "import spectre.Evolution.DgSubcell as scl\n",
    "import spectre.IO.H5 as h5\n",
    "from spectre import Interpolation\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "import matplotlib\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "\n",
    "# Set the path to the spectre bin directory. We will use this later for running executables.\n",
    "from pathlib import Path;\n",
    "spectre_bin_path = str(Path(spectre.__file__).parent.parent.parent)\n",
    "\n",
    "# Set up the path to the notebook. We assume input files are next to the notebook.\n",
    "import os\n",
    "notebook_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2bfa7-a5b9-470b-90fd-1d2dc9ea6b3e",
   "metadata": {},
   "source": [
    "# Overview and goals\n",
    "The primary goal is to solve a system of PDEs to a given accuracy. This means that both *too much* accuracy and *not enough* accuracy are\n",
    "bad. For time-dependent PDEs, like the Einstein equations, where we need more or less resolution to achieve the accuracy changes. This means\n",
    "we must adapt the mesh dynamically: adaptive mesh refinement (AMR). Interestingly, AMR often is used to reduce computational cost, not to\n",
    "increase resolution.\n",
    "\n",
    "## Domain decomposition\n",
    "We decompose the computational domain into a set of non-overlapping Elements.  In each Element the solution is represented at a set of grid\n",
    "points called the collocation points.  For a DG method we approximate the solution of the system as\n",
    "$$\n",
    "u(\\xi,t)\\approx\\sum_{i=0}^{N}u_i(t)\\ell_i(\\xi)=\\sum_{i=0}^{N}c_i(t)P_i(\\xi)\n",
    "$$\n",
    "where \n",
    "- $\\ell_i(\\xi)$ are Lagrange interpolating polynomials on the interval $\\xi \\in [-1, 1]$,\n",
    "- $u_i(t)$ the time-dependent pointwise value ofthe solution at the collocation points (nodal coefficients or values),\n",
    "- $P_i(\\xi)$ the Legendre polynomials,\n",
    "- and $c_i$ the spectral or modal coefficients.\n",
    "\n",
    "Using multiple Elements provides a natural way to parallelize the problem, and for complicated geometries such as those used\n",
    "in compact binary simulations, it is necessary to use multiple Elements to cover the computational domain.\n",
    "\n",
    "In order to achieve a given accuracy, we can adjust the resolution of a DG method in one of two ways.  The first is to change the number of\n",
    "collocation points in an Element. This is known a p-refinement as we are changing the order of our polynomial representation of the solution.\n",
    "\n",
    "Let's show the location of the collocation points as we increase $N \\in [2, 12]$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649aa64-5cf2-476b-993a-c5efd5848b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 13):\n",
    "    dg_mesh = Mesh[1](i, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "    dg_logical_coords = logical_coordinates(dg_mesh)\n",
    "    plt.plot(dg_logical_coords.get(0), np.full((dg_mesh.extents(0)), i), marker='o', linestyle='-')\n",
    "\n",
    "plt.ylabel(\"N\")\n",
    "plt.xlabel(r\"$\\xi$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7170f6-bd5b-44c4-bb00-9619f0c03735",
   "metadata": {},
   "source": [
    "Notice that the collocation points are not evenly distributed but clustered towards the endpoints of the Element.  The location of the collocation points is determined by the basis function (in this case Legendre polynomials) and the quadrature (in this case Gauss-Lobatto, which places collocation points on the boundary of the Element).  An alternative basis for DG is Chebyshev polynomials, and an alternative quadrature is Gauss which does not place collocation points on the boundary of the Element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58298ebe-30a4-450d-83d0-82c08c0eaef2",
   "metadata": {},
   "source": [
    "The second way to adjust the resolution is to change the number of Elements that are used to cover the computational domain. This is known as h-refinement as we are changing the size of the individual Elements with respect to the spatial scale of the problem.  We use a block-based h-refinement scheme where we decompose the computational domain into a set of non-overlapping Blocks.  Each Block is the coarsest Element in a refinement hierarchy.  In each dimension we define a refinement level by the number of times the Block has been split.\n",
    "\n",
    "Let's show the distribution of Elements as we increase the refinement level $L$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71518fb-5320-4c46-ba44-099123c47fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(0, 5):\n",
    "    num_pts = 2**l + 1\n",
    "    block_logical_coords = np.linspace(-1, 1, num = num_pts)\n",
    "    plt.plot(block_logical_coords, np.full((num_pts), l), marker='|', linestyle='-')\n",
    "\n",
    "plt.ylabel(\"L\")\n",
    "plt.xlabel(r\"$\\Xi$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fbd2a7-c7e2-4f55-8f7b-e0ce1bf342c7",
   "metadata": {},
   "source": [
    "### Multiple dimensions\n",
    "\n",
    "In multiple dimensions, we represent our DG solution as tensor products of polynomials:\n",
    "$$\n",
    "u(\\vec{\\xi},t) \\approx\\sum_{i=0}^{N_{\\xi}}\\sum_{j=0}^{N_{\\eta}}\\sum_{k=0}^{N_{\\zeta}}u_{ijk}(t)\\ell_i(\\xi)\\ell_j(\\eta)\\ell_k(\\zeta)\n",
    "= \\sum_{i=0}^{N_{\\xi}}\\sum_{j=0}^{N_{\\eta}}\\sum_{k=0}^{N_{\\zeta}}c_{ijk}(t)P_i(\\xi)P_j(\\eta)P_k(\\zeta)\n",
    "$$\n",
    "where $\\vec{\\xi} = (\\xi,\\eta,\\zeta)$ are the element logical coordinates that cover the Element in $\\xi^i \\in [-1, 1]$.\n",
    "\n",
    "For complicated geometries of the computational domain it is necessary to use multiple distorted Blocks that are then subdivided into Elements.  Each Block is a d-dimensional hypercube with block logical coordinates $\\vec{\\Xi}$ that cover the block in $\\Xi^i \\in [-1, 1]$. \n",
    "We can uniquely identify each Element by specifying $\\{B, \\vec{L}, \\vec{I}\\}$ where $B$ is label of the $B$-th Block, $\\vec{L}$ is the refinement level of the Element in each dimension, and $\\vec{I}$ is the index $I^i \\in [0, 2^{L^i} -1]$ labeling the segment of the refinement level covered by the Element in each dimension (with $I^i = 0$ at the lower end of the Block).  \n",
    "\n",
    "This then defines a simple linear mapping from the element logical coordinates $\\vec{\\xi}_K$ of the $K$-th Element to the block logical coordinates $\\vec{\\Xi}$ of its parent Block:\n",
    "$$\n",
    "\\Xi^i = h^i_K \\xi^i_K + b^i_K - 1\n",
    "$$\n",
    "where $h^i_K = 2^{-L^i_K}$, and $b^i_K = h^i_K*(2*I^i_K + 1)$.\n",
    "\n",
    "Then in order to cover the computational domain, we can define a mapping from the block logical cooordinates of each Block to the coordinates $\\vec{x}$ with which we define the computational domain:\n",
    "$$\n",
    "\\vec{x} = \\mathcal{M}_B(\\vec{\\Xi}_B)\n",
    "$$\n",
    "which deforms the hypercube of the Block into a distorted shape\n",
    "\n",
    "![Binary domain](../01_Monday/images/domain.png \"Binary domain\")\n",
    "\n",
    "In multiple dimensions we can adjust the number of collocation points and refinement level in each dimension independently; this is known as anisotropic refinement.  If we demand that each dimension has the same number of collocation points or refinement level, this is known as isotropic refinement.  \n",
    "\n",
    "For the remainder of this tutorial, we will stick to one spatial dimension.\n",
    "\n",
    "Let's look at the spectral coefficients for $u = e^{-\\xi}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d854c6-b78c-4391-8734-125c6b1251d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16, 1, -1):\n",
    "    dg_mesh = Mesh[1](i, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "    dg_logical_coords = logical_coordinates(dg_mesh)\n",
    "    soln = DataVector(np.exp(-1.0*(dg_logical_coords.get(0))))\n",
    "    plt.semilogy(np.arange(0, dg_mesh.extents(0)),\n",
    "                 power_monitors(soln, dg_mesh)[0], marker='o', linestyle='',\n",
    "                 label=str(i))\n",
    "    \n",
    "plt.ylabel(\"Power $|c_i|$\")\n",
    "plt.xlabel(\"Spectral coefficient index $i$\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8afe6b-27c1-4614-9aa1-20b50fae19bf",
   "metadata": {},
   "source": [
    "We see that the coefficients exponentially decay with increasing resolution.\n",
    "\n",
    "Now let's look at how the error in interpolating $e^{-\\xi}$ to an arbitrary point decays with increasing resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e6dcb-591d-4ab7-8067-0b538d9686a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_logical_coords = [DataVector(1, 0.1)]\n",
    "exact = DataVector(1, np.exp(-0.1))\n",
    "for i in range(16, 1, -1):\n",
    "    dg_mesh = Mesh[1](i, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "    dg_logical_coords = logical_coordinates(dg_mesh)\n",
    "    soln = DataVector(np.exp(-1.0*(dg_logical_coords.get(0))))\n",
    "    interpolator = Interpolation.Irregular1D(dg_mesh, target_logical_coords=[DataVector(1, 0.1)])\n",
    "    interpolated = interpolator.interpolate(soln)\n",
    "    error = np.abs(interpolated - exact)\n",
    "    plt.plot(i, error, marker='o')\n",
    "\n",
    "plt.ylabel(\"Interpolation error\")\n",
    "plt.xlabel(\"N\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978f263-bafa-4c9d-9c75-0fcfd041d7bf",
   "metadata": {},
   "source": [
    "We see the interpolation error also falls off exponentially.\n",
    "\n",
    "Now let us examine how the interpolation falls off for fixed polynomial order, but we increase the refinement level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefde6c-c035-4d58-85ed-8b97359f7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_block_logical_coords = [DataVector(1, 0.1)]\n",
    "exact = DataVector(1, np.exp(-0.1))\n",
    "for i in range(8, -1, -1):\n",
    "    dg_mesh = Mesh[1](5, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "    m = 2**(-i)\n",
    "    element_index = 0\n",
    "    target_el_coord = 1.1/m - 2*element_index - 1\n",
    "    while target_el_coord > 1 and element_index < 2**i:\n",
    "        element_index += 1\n",
    "        target_el_coord -= 2\n",
    "\n",
    "    dg_element_logical_coords = logical_coordinates(dg_mesh)\n",
    "    dg_block_logical_coords = m*(dg_element_logical_coords.get(0) + 2*element_index + 1) - 1\n",
    "    soln = DataVector(np.exp(-1.0*(dg_block_logical_coords)))\n",
    "    interpolator = Interpolation.Irregular1D(dg_mesh, target_logical_coords=[DataVector(1, target_el_coord)])\n",
    "    interpolated = interpolator.interpolate(soln)\n",
    "    error = np.abs(interpolated - exact)\n",
    "    plt.plot(i, error, marker='o')\n",
    "\n",
    "plt.ylabel(\"Interpolation error\")\n",
    "plt.xlabel(\"L\")\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1baa4-6003-4142-b3d3-8b38d44cdf7a",
   "metadata": {},
   "source": [
    "## The solution and what accuracy means\n",
    "Since we wish to achieve a given level of accuracy, we must define what we actually mean by that. Again, for a DG method we approximate the solution of the system as\n",
    "$$\n",
    "u(\\xi,t)\\approx\\sum_{i=0}^{N}u_i(t)\\ell_i(\\xi)=\\sum_{i=0}^{N}c_i(t)P_i(\\xi)\n",
    "$$\n",
    "where $\\ell_i(\\xi)$ are Lagrange interpolating polynomials, $u_i(t)$ the time-dependent pointwise value of the solution at the grid points (nodal coefficients or values), $P_i(\\xi)$ the Legendre polynomials, and $c_i$ the spectral or modal coefficients.\n",
    "\n",
    "Since we can either change the number of modes (p-refinement) or split/join the element(s) (h-refinement), we need a way of deciding which to do. We can divide this into two tasks (1) decide whether we are accurate enough or not (2) decide if we need to change the mesh based on that decision.\n",
    "\n",
    "In order to decide whether or not we are accurate enough, we need to estimate the error in our solution. Let's define the solution $\\hat{u}(\\xi,t)$ as\n",
    "$$\n",
    "\\hat{u}(\\xi,t)\\approx\\sum_{i=0}^{N-1}u_i(t)\\ell_i(\\xi).\n",
    "$$\n",
    "That is, it is our solution but with one fewer grid points inside the element. We can then compute $\\mathcal{E}_N\\approx|u(\\xi,t)-\\hat{u}(\\xi,t)|$ in the element to estimate the truncation error. If we look at the modal expansion we see that this is actually just $|c_NP_N(\\xi)|$. This means we can use the amount of power in the highest (or several highest) modes to estimate the truncation error of the solution. In SpECTRE this is called the `TruncationError` [AMR criterion](https://spectre-code.org/classamr_1_1Criteria_1_1TruncationError.html). By comparing the truncation error estimate to our target truncation error (\"target accurracy\") we can decide whether or not the solution is accurate enough or too accurate. In the case that the solution is not accurate enough, we must add a grid point to the element to reduce the truncation error.\n",
    "\n",
    "If the solution is too accurate, whether or not to coarsen/remove a grid point is more difficult. This is because if we remove a grid point, our solution may then no longer be accurate enough. The next time AMR runs it would then decide to add a grid point, which would then make the solution too accurate, and so it would remove the grid point again later, etc. This sort of \"flip-flopping\" back and forth between resolutions is bad since it creates a lot of noise and also means that sometimes we would be below our target truncation error. Instead, we define another solution $\\bar{u}(\\xi,t)$ as\n",
    "$$\n",
    "\\bar{u}(\\xi,t)\\approx\\sum_{i=0}^{N-2}u_i(t)\\ell_i(\\xi).\n",
    "$$\n",
    "Using the same procedure as before, we can estimate the truncation error of this solution by computing $\\mathcal{E}_{N-1}\\approx|\\hat{u}(\\xi,t)-\\bar{u}(\\xi,t)|$. If $\\mathcal{E}_{N-1}$ is greater than our target truncation error then we stay at the current resolution. If it is below the target truncation error, then we can safely use one fewer grid points. Of course, this is only one possible approach. Another simpler approach would be to only coarsen if $\\mathcal{E}_N$ is $K$ times smaller than the target truncation error.\n",
    "\n",
    "### Example: p-refinement\n",
    "\n",
    "To get some intuition, let's look at an example. We will start by creating 3 meshes with 8, 9, and 10 grid points and then evaluating the function $\\sin(2 \\xi+1.83) + \\xi$ on the corresponding grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14037d37-9b86-44a7-af28-4afbd631385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_mesh_10 = Mesh[1](10, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "dg_mesh_9 = Mesh[1](9, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "dg_mesh_8 = Mesh[1](8, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "\n",
    "dg_logical_coords_10 = logical_coordinates(dg_mesh_10)\n",
    "dg_logical_coords_9 = logical_coordinates(dg_mesh_9)\n",
    "dg_logical_coords_8 = logical_coordinates(dg_mesh_8)\n",
    "soln_a10 = DataVector(np.sin(2.0 * dg_logical_coords_10.get(0) + 1.83) + dg_logical_coords_10.get(0))\n",
    "soln_a9 = DataVector(np.sin(2.0 * dg_logical_coords_9.get(0) + 1.83) + dg_logical_coords_9.get(0))\n",
    "soln_a8 = DataVector(np.sin(2.0 * dg_logical_coords_8.get(0) + 1.83) + dg_logical_coords_8.get(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984954d1-56b2-4749-974f-913089b761ae",
   "metadata": {},
   "source": [
    "Now let's plot the solutions to see if we can visually see a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d53b6-b674-4863-8e4f-5086d3497d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dg_logical_coords_10.get(0), soln_a10, marker='o', linestyle='', label=\"10\")\n",
    "plt.plot(dg_logical_coords_9.get(0), soln_a9, marker='o', linestyle='', label=\"9\")\n",
    "plt.plot(dg_logical_coords_8.get(0), soln_a8, marker='o', linestyle='', label=\"8\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a8b33-f390-4db0-a91a-febb12986257",
   "metadata": {},
   "source": [
    "The solutions are clearly very similar and we can't visually tell the difference. Now let's plot the absolute value of the spectral coefficients to see how rapidly they decay and to get some estimate of what the solution error actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd0b2f-817d-4b72-a711-d4ddd8d6da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.arange(1, dg_mesh_10.extents(0)+1),\n",
    "             power_monitors(soln_a10, dg_mesh_10)[0], marker='o', linestyle='',\n",
    "            label=\"10\")\n",
    "plt.semilogy(np.arange(1, dg_mesh_9.extents(0)+1),\n",
    "             power_monitors(soln_a9, dg_mesh_9)[0], marker='o', linestyle='',\n",
    "            label=\"9\")\n",
    "plt.semilogy(np.arange(1, dg_mesh_8.extents(0)+1),\n",
    "             power_monitors(soln_a8, dg_mesh_8)[0], marker='o', linestyle='',\n",
    "            label=\"8\")\n",
    "plt.ylabel(\"Power $|c_i|$\")\n",
    "plt.xlabel(\"Spectral coefficient index $i$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599a693-8095-469d-847c-9d913862826c",
   "metadata": {},
   "source": [
    "From this we see that the solution error is $\\mathcal{E}_N\\approx10^{-6}$ and $\\mathcal{E}_{N-1}\\approx10^{-4}$. Let's interpolate the solutions on the 8 and 9 point grids to the 10 point grid so that we can compute the differences. We can use SpECTRE's interpolation code to interpolate the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911b21d-faeb-414a-952b-ae77df120d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolator8_to_10 = Interpolation.RegularGrid1D(dg_mesh_8, dg_mesh_10)\n",
    "interpolator9_to_10 = Interpolation.RegularGrid1D(dg_mesh_9, dg_mesh_10)\n",
    "\n",
    "soln_a8_10 = interpolator8_to_10.interpolate(soln_a8)\n",
    "soln_a9_10 = interpolator9_to_10.interpolate(soln_a9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ed5e5-9be0-43fa-8138-69e758a8a999",
   "metadata": {},
   "source": [
    "As a sanity check, let's plot all 3 solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00cace9-b32f-43ab-8f56-5a16c31c3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dg_logical_coords_10.get(0), soln_a10, marker='o', linestyle='', label=\"10\")\n",
    "plt.plot(dg_logical_coords_10.get(0), soln_a9_10, marker='o', linestyle='', label=\"9\")\n",
    "plt.plot(dg_logical_coords_10.get(0), soln_a8_10, marker='o', linestyle='', label=\"8\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072587b5-2f1c-427d-8fa1-cba6b11e1526",
   "metadata": {},
   "source": [
    "All 3 solutions are visually indistinguishable, as we would expect from our error estimates above. Now let's plot $|u-\\hat{u}|$ and $|\\hat{u}-\\bar{u}|$ on a semi-log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19929aff-7d8b-4579-8240-b799b7ee0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dg_logical_coords_10.get(0), np.abs(soln_a10 - soln_a9_10), marker='o', linestyle='', label=\"10-9\")\n",
    "plt.plot(dg_logical_coords_10.get(0), np.abs(soln_a9_10 - soln_a8_10), marker='o', linestyle='', label=\"9-8\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb04b8-c866-46a6-ac02-52050909c72a",
   "metadata": {},
   "source": [
    "We see that our estimate of $\\mathcal{E}_N\\approx10^{-6}$ and $\\mathcal{E}_{N-1}\\approx10^{-4}$ are both quite good. This gives us some confidence in our truncation error estimation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9997635-bff2-4789-a4f1-3fdb59bbb3f1",
   "metadata": {},
   "source": [
    "### Example: h-refinement\n",
    "Now we turn to the more challenging task of deciding when to h-refine instead of p-refine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e3dfa-7da8-485d-974d-e9d74c23491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(1, dg_mesh_10.extents(0)+1)\n",
    "plt.semilogy(n, 1/n**4, '--', label=f\"$1 / (i+1)^{i}$\")\n",
    "\n",
    "plt.semilogy(np.arange(1, dg_mesh_10.extents(0)+1),\n",
    "             power_monitors(soln_a10, dg_mesh_10)[0], marker='o', linestyle='',\n",
    "            label=\"10\")\n",
    "plt.semilogy(np.arange(1, dg_mesh_9.extents(0)+1),\n",
    "             power_monitors(soln_a9, dg_mesh_9)[0], marker='o', linestyle='',\n",
    "            label=\"9\")\n",
    "plt.semilogy(np.arange(1, dg_mesh_8.extents(0)+1),\n",
    "             power_monitors(soln_a8, dg_mesh_8)[0], marker='o', linestyle='',\n",
    "            label=\"8\")\n",
    "plt.ylabel(\"Power $|c_i|$\")\n",
    "plt.xlabel(\"Spectral coefficient index $i$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d8c73-a882-481c-92b5-a9eab7baad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_mesh_7 = Mesh[1](7, Basis.Legendre, Quadrature.GaussLobatto)\n",
    "dg_logical_coords_7 = logical_coordinates(dg_mesh_7)\n",
    "soln_a7 = DataVector(np.sin(2.0 * dg_logical_coords_7.get(0) + 1.83) + dg_logical_coords_7.get(0))\n",
    "\n",
    "print(\"Resolution 8 series converges: \", not scl.persson_tci(Scalar[DataVector](soln_a8), dg_mesh_8, 4.0, 1))\n",
    "print(\"Resolution 7 series converges: \", not scl.persson_tci(Scalar[DataVector](soln_a7), dg_mesh_7, 4.0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646cce0-e72a-420c-b581-4028f34f8613",
   "metadata": {},
   "source": [
    "**Note**: Lesson here is that if you start at low resolution, you will over h-refine, then p-refine, and the possible h-coarsen and then more p-refine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e801e-eacf-4d64-9234-2b2e1f8b1e8e",
   "metadata": {},
   "source": [
    "## Simulation example: Scalar Waves in 1D\n",
    "Let's see things in action.  We will evolve a scalar wave in 1D, allowing the grid to p-refine based on the TruncationError AMR criterion.\n",
    "The scalar wave system is the solution to:\n",
    "$$\n",
    "\\frac{\\partial^2 \\Psi}{\\partial t^2} = \\nabla^2 \\Psi\n",
    "$$\n",
    "It is straightforward to show that a plane wave is a solution to the wave equation, given by\n",
    "$$\n",
    "\\Psi(\\vec{x},t) = F(u(\\vec{x},t))\n",
    "$$\n",
    "where the profile $F$ of the plane wave is an arbitrary one-dimensional function of \n",
    "$$\n",
    "u = \\vec{k} \\cdot (\\vec{x} - \\vec{x_c}) - \\omega t\n",
    "$$\n",
    "with the wave vector $\\vec{k}$, the frequency $\\omega = ||\\vec{k}||$ and initial center of the profile $\\vec{x_c}$.\n",
    "\n",
    "In order to solve the system in first-order form, we introduce the auxiliary variables $\\Pi = -\\partial_t \\Psi$ and $\\Phi_i = \\partial_i \\Psi$ in order to get the first-order system:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\partial_t \\Psi &= - \\Pi \\\\\n",
    "\\partial_t \\Pi &=  -\\partial^i \\Phi_i \\\\\n",
    "\\partial_t \\Phi_i &= -\\partial_i \\Pi\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We solve this system on the interval $x \\in [1, 3]$ imposing analytic boundary conditions for a plane wave with a wave vector $k = 1$, centered at $x_c = 0$, and with a Gaussian profile\n",
    "$$\n",
    "F(u) = A \\exp\\left(-\\frac{(u-c)^2}{w^2}\\right)\n",
    "$$\n",
    "with amplitude $A = 1$, center $c = -6$ and width $w = 1$.\n",
    "\n",
    "Initially (at $t=0$), the scalar field will be effectively zero on the interval, but as the system is evolved, a Gaussian pulse will enter the domain from the left and propagate at the speed of light to the right. \n",
    "\n",
    "The interval is split into four elements, each with an initial resolution of 4 grid points.  \n",
    "\n",
    "We will now evolve the system from $t = 0$ until $t = 15$ with a time-step of $\\Delta t = 0.001$ allowing the mesh to be refined every $\\Delta t_{AMR} = 0.05$ with a target truncation error of $\\mathcal{E}_N = 10^{-6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e0e1e-dfbe-418b-8ca1-8d25086e5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "! $spectre_bin_path/EvolveScalarWave1D --input-file=$notebook_path/Sw1D.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bca30-719d-4195-aac6-90f327833d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_file = h5.H5File(\"Volume0.h5\")\n",
    "volume_data = volume_file.get_vol(\"Fields.vol\")\n",
    "obs_ids = volume_data.list_observation_ids()\n",
    "coords = [np.asarray(volume_data.get_tensor_component(obs_ids[obs_int], \"InertialCoordinates_x\").data).copy()\n",
    "                  for obs_int in range(len(obs_ids))]\n",
    "psi = [np.asarray(volume_data.get_tensor_component(obs_ids[obs_int], \"Psi\").data).copy()\n",
    "                  for obs_int in range(len(obs_ids))]\n",
    "times = [volume_data.get_observation_value(obs_ids[obs_int])\n",
    "              for obs_int in range(len(obs_ids))]\n",
    "def plot_step_at(obs_int):\n",
    "    plt.cla()\n",
    "    plt.plot(coords[obs_int], psi[obs_int], marker='o', linestyle='')\n",
    "    plt.title(f\"Time {times[obs_int]:1.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442db23d-3549-48ac-9a4f-891b40a6cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "matplotlib.animation.FuncAnimation(fig, plot_step_at, frames=len(psi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abccf6c-c433-45dc-9e6d-9eeaaa4eadef",
   "metadata": {},
   "source": [
    "Note that at the beginning the grid becomes coarser so that each element has only two grid points which is the minimum allowed resolution for our choice of basis function (Legendre polynomials) and quadrature (Gauss-Lobatto).  As the Gaussian pulse enters the grid, the resolution increases, and after the pulse leaves the grid, the resolution again coarsens.\n",
    "\n",
    "\n",
    "Now let us plot the power monitors (the absolute values of the spectral coefficients $c_i$) for each element on a semi-log scale (shown on the left) overlayed on the evolved scalar field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9a6cb-8eb9-4b3c-b884-3f804655fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_data_by_element = volume_data.get_data_by_element(None, None, [\"Psi\", \"InertialCoordinates_x\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx() \n",
    "def plot_sw_power_monitors(obs_int):\n",
    "    ax.cla()\n",
    "    ax2.cla()\n",
    "    data_at_this_time = sw_data_by_element[obs_int][2]\n",
    "    for element_data in data_at_this_time:\n",
    "        coords = None\n",
    "        psi = None\n",
    "        for tensor_component in element_data.tensor_components:\n",
    "            if tensor_component.name == \"Psi\":\n",
    "                psi = DataVector(tensor_component.data)\n",
    "            elif tensor_component.name == \"InertialCoordinates_x\":\n",
    "                coords = DataVector(tensor_component.data)\n",
    "        element_mesh = Mesh[1](element_data.extents[0], element_data.basis[0], element_data.quadrature[0])\n",
    "        psi_power_monitor = power_monitors(psi, element_mesh)[0]\n",
    "        ax.semilogy(coords, psi_power_monitor, marker='o', linestyle='')\n",
    "        ax2.plot(coords, psi)\n",
    "    ax.set_ylim(1e-10, 1)\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.title(f\"Time {sw_data_by_element[obs_int][1]:1.2f}\")\n",
    "matplotlib.animation.FuncAnimation(fig, plot_sw_power_monitors, frames=len(sw_data_by_element))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
